{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7caf781-7131-4b4e-b80b-69f78a727bfb",
   "metadata": {},
   "source": [
    "# Data selection based on OT gradients\n",
    "- **This notebook uses 'cola' sub-task from GLUE benchmark for demonstration. The process for other tasks are essentially the same.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f448b146-0e54-4634-9701-a702368710e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5fedfb-2a82-4a28-83a7-e4f2e0732b2b",
   "metadata": {},
   "source": [
    "Load samples from selected domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7cfe86-8705-4bbb-86b0-1a7b5d0f9782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./llm_datasets/amz_0to80.pkl', 'rb') as file:\n",
    "    # Load the object from the file\n",
    "    amz_full = pickle.load(file)\n",
    "\n",
    "with open('./llm_datasets/news_full_0to80.pkl', 'rb') as file:\n",
    "    # Load the object from the file\n",
    "    news_full = pickle.load(file)\n",
    "\n",
    "with open('./llm_datasets/real_books_2M.pkl', 'rb') as file:\n",
    "    # Load the object from the file\n",
    "    realb_full = pickle.load(file)\n",
    "\n",
    "with open('./llm_datasets/ready1k_owtc_3M.pkl', 'rb') as file:\n",
    "    # Load the object from the file\n",
    "    owtc_full = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d89612-8f83-4e60-b5be-e987bb9547dc",
   "metadata": {},
   "source": [
    "Construct a candidate dataset of 5M samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a392bf89-cf6a-43f0-a0ea-3d3904a9293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sp_4d = np.concatenate([amz_full[:1500000], realb_full[:1000000], news_full[:1500000], owtc_full[:1000000]], axis = 0)\n",
    "sp_4d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0ee485-e2a8-4828-8734-e99bd06d3237",
   "metadata": {},
   "source": [
    "Samples are processed in the same way as detailed in the pre-processing notebook. Here we directly load the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40aae79-d32d-4e5e-a691-b297a467ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./embeds_news_f.pkl', 'rb') as file:\n",
    "    # Load the object from the file\n",
    "    embeds_news = pickle.load(file)\n",
    "    \n",
    "with open('./embeds_owtc_f.pkl', 'rb') as file:\n",
    "    # Load the object from the file\n",
    "    embeds_owtc = pickle.load(file)\n",
    "    \n",
    "with open('./embeds_amz_f.pkl', 'rb') as file:\n",
    "    # Load the object from the file\n",
    "    embeds_amz = pickle.load(file)\n",
    "\n",
    "with open('./embeds_book1.pkl', 'rb') as file:\n",
    "    # Load the object from the file\n",
    "    embeds_book1 = pickle.load(file)\n",
    "\n",
    "embeds_4d = np.concatenate([embeds_amz[:1500000], embeds_book1[:1500000], embeds_news[:1000000], embeds_owtc[:1000000]], axis = 0)\n",
    "embeds_4d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f60a42-5a87-4473-b1d4-01575f379b1d",
   "metadata": {},
   "source": [
    "Load 'jax' for GPU computation of OT using 'ott-jax' package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23069bc4-83cc-40dc-8410-8fe604a2da01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import os\n",
    "\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = \"false\"\n",
    "\n",
    "print(jax.numpy.ones(3).device()) # TFRT_CPU_0\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import ott\n",
    "from ott.geometry import pointcloud\n",
    "from ott.problems.linear import linear_problem\n",
    "from ott.solvers.linear import sinkhorn, sinkhorn_lr\n",
    "from ott.tools import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d5ea02-d65b-45e9-bd72-b0d8eb5af382",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'cola'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a5a43-9860-4c82-9b4a-84afdc67c7cb",
   "metadata": {},
   "source": [
    "**Solve the OT problem between the embeddings of target task data and candidate data.** We use the 'batch-wise' method to deal with the high memory demand from the problem size. We use 'momentum acceleration' and 'entropy regularization' to speed up the solution process while maintaining its numerical stability. The parameters for these two techniques need to be tuned together to achieve optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ba33be-2337-4f6e-8bd5-16fae5b7cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ott.solvers.linear import sinkhorn_lr\n",
    "import tqdm\n",
    "from ott import utils\n",
    "from ott.solvers.linear import acceleration\n",
    "\n",
    "batch_size = 2000\n",
    "\n",
    "geom = pointcloud.PointCloud(np.array(cola_1000), np.array(embeds_4d), epsilon=1e-1, batch_size = batch_size)\n",
    "ot_prob = linear_problem.LinearProblem(geom)\n",
    "\n",
    "with tqdm.tqdm() as pbar:\n",
    "    progress_fn = utils.tqdm_progress_fn(pbar)\n",
    "    solver = sinkhorn.Sinkhorn(progress_fn=progress_fn, momentum=acceleration.Momentum(value=1.3), threshold = 1e-1, inner_iterations=1, max_iterations = 2000)\n",
    "    # solver = sinkhorn.Sinkhorn(progress_fn=progress_fn, inner_iterations=1, max_iterations = 200000)\n",
    "    # momentum=None\n",
    "    ot_u = jax.jit(solver)(ot_prob)\n",
    "\n",
    "print(f\"Converged: {ot_u.converged}, cost: {ot_u.reg_ot_cost}\")\n",
    "\n",
    "    # transp_cost = ot_lr.compute_reg_ot_cost(ot_prob, use_danskin=True)\n",
    "transp_cost = ot_u.reg_ot_cost\n",
    "transp_cost\n",
    "\n",
    "with open('otg_' + task + '.pkl', 'wb') as file:\n",
    "    pickle.dump(ot_u.g, file, protocol=4)\n",
    "    #     pickle.dump(embeddings_tensor, file, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1de834-f29a-42fa-8802-498934a5acd9",
   "metadata": {},
   "source": [
    "Compute the calibrated gradient based on [LAVA: Data Valuation without Pre-Specified Learning Algorithms, ICLR 2023]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ad978f-d3b3-4ba7-bf53-be311c93314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsP = np.array(ot_u.g)\n",
    "mean_all = np.sum(gsP) / len(gsP)\n",
    "gsP = gsP - (mean_all - gsP / len(gsP))\n",
    "print(gsP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b552286-5afe-4da2-bcc0-d1c618504bf9",
   "metadata": {},
   "source": [
    "Rank all the candidate samples based on the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93863f93-3e04-409d-a41f-46e1c1413f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "g_series = pd.Series(gsP)\n",
    "ranked_indices = g_series.rank(ascending=True).argsort()\n",
    "ranked_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80214a6c-a8a8-4bc6-8b7b-43b0e06d27c1",
   "metadata": {},
   "source": [
    "Show a few samples and save the selected data. Then the process is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94625440-126c-471c-929d-3d8187225877",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_4d[ranked_indices[0:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd03f7ac-0be9-4ef1-b6f9-d5b8b4e22232",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cola_ot_5m_150k.pkl', 'wb') as file:\n",
    "    pickle.dump(sp_4d[ranked_indices[:150000]], file, protocol=4)\n",
    "    #     pickle.dump(embeddings_tensor, file, protocol=4)\n",
    "\n",
    "with open('cola_ot_5m_300k.pkl', 'wb') as file:\n",
    "    pickle.dump(sp_4d[ranked_indices[:300000]], file, protocol=4)\n",
    "    #     pickle.dump(embeddings_tensor, file, protocol=4)\n",
    "\n",
    "with open('cola_ot_5m_500k.pkl', 'wb') as file:\n",
    "    pickle.dump(sp_4d[ranked_indices[:500000]], file, protocol=4)\n",
    "    #     pickle.dump(embeddings_tensor, file, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adcf39c-18cb-4409-8609-749e5d3776d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d757c87c-9e0c-4e22-9a9e-2818ba1de580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p39h",
   "language": "python",
   "name": "p39h"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

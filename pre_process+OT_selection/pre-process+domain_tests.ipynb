{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0da4292-bcc9-4740-b307-9c331e7732f5",
   "metadata": {},
   "source": [
    "# Data pre-processing and domain similarity tests\n",
    "- **This notebook uses 'cola' sub-task from GLUE benchmark for demonstration. The process for other tasks are essentially the same.**\n",
    "\n",
    "**Pre-processing** is first performed for both target task samples and candidate samples where we normalize all samples to a fixed length of 1000 characters to avoid different padding patterns affect the analysis on distributional distances.\n",
    "\n",
    "- For samples with lengths much shorter than 1000 characters (e.g., training data for 'cola'), we concatenate multiple samples to reach 1000 characters; for samples much longer than 1000 characters (e.g., scientific papers), we split each of the original samples to multiple samples of 1000 characters.\n",
    "\n",
    "- Then, we tokenize the processed samples using BERT tokenizers and embed the tokens using distilledBERT fine-tuned on the target task.\n",
    "\n",
    "For **domain similarity tests**, we randomly sample 20k samples from each of the 7 domains in the candidate data ['amz', 'wiki', 'news', 'pubmed', 'arxiv', 'book1', 'owtc']. We then tokenize and embed these samples in the same way.\n",
    "\n",
    "- To analyze the domain similarity, we compute the OT distance between the embeddings of target task samples and the embeddings of samples from each domain.\n",
    "\n",
    "- Then, we select 2~4 domains with the smallest OT distances to the target task data and use samples from these domains as the candidate data for further selection. Domains with large OT distances will be discarded for this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb416d20-c5ce-42c5-ab00-f7c5a7456563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024af2fc-653c-4dcf-b0a3-af7543ad9f20",
   "metadata": {},
   "source": [
    "**sample pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2271d9e4-695c-4f43-9455-d12d600b5c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the CoLA dataset\n",
    "dataset = load_dataset(\"glue\", \"cola\")\n",
    "\n",
    "# Print the train samples\n",
    "print(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e94f0ef-64f3-4989-9b4e-240c14833c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "cola_sp = np.array(dataset[\"train\"]['sentence'])\n",
    "cola_sp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad83bc75-b040-49c1-9930-9abd5d43faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "len1k = []\n",
    "current_text = ''\n",
    "for text in cola_sp:\n",
    "    current_text = current_text + text\n",
    "    if len(current_text) >= 1000:\n",
    "        len1k.append(current_text[:1000])\n",
    "        current_text = ''\n",
    "        \n",
    "len(len1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102fe3cd-c4e2-47c3-af45-f352dcf19aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cola_1000 = len1k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6747ef-85d9-487e-b449-3adccfd0dc79",
   "metadata": {},
   "source": [
    "tokenizing use BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1593595b-6275-4fd9-b6d1-041427f80e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokens_cola_1000 = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "for text in tqdm(np.concatenate(cola_1000, axis=0).tolist(), desc=\"Processing sentences\"):\n",
    "    tokens = tokenizer.encode_plus(text, add_special_tokens=True, padding='max_length', truncation=True, max_length=295, return_tensors='pt')\n",
    "    # tokens_tensor = torch.from_numpy(tokens)\n",
    "    # tokens_tensor = tokens_tensor.to('cuda')\n",
    "    tokens = {key: value.to(device) for key, value in tokens.items()}\n",
    "    tokens_cola_raw.append(tokens)\n",
    "\n",
    "with open('tokens_cola_1000.pkl', 'wb') as file:\n",
    "    pickle.dump(tokens_cola_1000, file, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf04ce75-762c-4668-a7c0-998a874f2092",
   "metadata": {},
   "source": [
    "embedding with distilledBERT fine-tuned on the target task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b071597d-fc74-4c60-83db-81ea06332b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "input_ids = torch.stack([item['input_ids'] for item in tokens_cola_1000]).squeeze()\n",
    "attention_mask = torch.stack([item['attention_mask'] for item in tokens_cola_1000]).squeeze()\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_mask)\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "from transformers import DistilBertConfig, DistilBertModel\n",
    "\n",
    "\n",
    "# Load the configuration from a json file (if you have it)\n",
    "config = DistilBertConfig.from_json_file('./output10e5/config.json')\n",
    "# Load the model weights from the .bin file\n",
    "model = DistilBertModel.from_pretrained('./output10e5/pytorch_model.bin', config=config)\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "embeddings_list = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for batch in tqdm(dataloader, desc=\"Processing sentences\"):\n",
    "    batch_input_ids, batch_attention_mask = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=batch_input_ids.to(device), \n",
    "                        attention_mask=batch_attention_mask.to(device))\n",
    "\n",
    "    # embeddings = outputs.last_hidden_state.cpu().detach().numpy()\n",
    "    embeddings = outputs.last_hidden_state.cpu().detach().numpy()\n",
    "    embeddings_list.append(np.mean(embeddings, axis=1))\n",
    "\n",
    "# Concatenate all the embeddings if needed\n",
    "# embeddings_tensor = torch.cat(embeddings_list, dim=0)\n",
    "embeddings_tensor = np.concatenate(embeddings_list, axis=0)\n",
    "\n",
    "with open('embeds_cola_1000.pkl', 'wb') as file:\n",
    "    pickle.dump(embeddings_tensor, file, protocol=4)\n",
    "    #     pickle.dump(embeddings_tensor, file, protocol=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89376aa0-7f77-413e-9be2-d65f75b65e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c275c4e-23d8-46c7-8bb9-c92fce702637",
   "metadata": {},
   "source": [
    "'rand7_20k' contains 20k samples from each of the 7 domains: ['amz', 'wiki', 'news', 'pubmed', 'arxiv', 'book1', 'owtc'].\n",
    "\n",
    "Samples are processed in the same way as cola to have a fixed length of 1000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd03a48-f105-41ae-8a0e-802cf1b04ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokens_rand7_20k = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "for text in tqdm(rand7_20k.tolist(), desc=\"Processing sentences\"):\n",
    "    tokens = tokenizer.encode_plus(text, add_special_tokens=True, padding='max_length', truncation=True, max_length=295, return_tensors='pt')\n",
    "    # tokens_tensor = torch.from_numpy(tokens)\n",
    "    # tokens_tensor = tokens_tensor.to('cuda')\n",
    "    tokens = {key: value.to(device) for key, value in tokens.items()}\n",
    "    tokens_rand7_20k.append(tokens)\n",
    "    \n",
    "with open('tokens_rand7_20k.pkl', 'wb') as file:\n",
    "    pickle.dump(tokens_rand7_20k, file, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7408d80-c95a-485f-b69a-fb3a529dcd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "input_ids = torch.stack([item['input_ids'] for item in tokens_rand7_20k]).squeeze()\n",
    "attention_mask = torch.stack([item['attention_mask'] for item in tokens_rand7_20k]).squeeze()\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_mask)\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "from transformers import DistilBertConfig, DistilBertModel\n",
    "\n",
    "\n",
    "# Load the configuration from a json file (if you have it)\n",
    "config = DistilBertConfig.from_json_file('./output10e5/config.json')\n",
    "# Load the model weights from the .bin file\n",
    "model = DistilBertModel.from_pretrained('./output10e5/pytorch_model.bin', config=config)\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "embeddings_list = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for batch in tqdm(dataloader, desc=\"Processing sentences\"):\n",
    "    batch_input_ids, batch_attention_mask = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=batch_input_ids.to(device), \n",
    "                        attention_mask=batch_attention_mask.to(device))\n",
    "\n",
    "    # embeddings = outputs.last_hidden_state.cpu().detach().numpy()\n",
    "    embeddings = outputs.last_hidden_state.cpu().detach().numpy()\n",
    "    embeddings_list.append(np.mean(embeddings, axis=1))\n",
    "\n",
    "# Concatenate all the embeddings if needed\n",
    "# embeddings_tensor = torch.cat(embeddings_list, dim=0)\n",
    "embeddings_tensor = np.concatenate(embeddings_list, axis=0)\n",
    "\n",
    "with open('embeds_rand7_20k.pkl', 'wb') as file:\n",
    "    pickle.dump(embeddings_tensor, file, protocol=4)\n",
    "    #     pickle.dump(embeddings_tensor, file, protocol=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec11def-9b1b-43c5-bc7c-50f0a4cdab6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbdcd5f4-096e-4e3a-b1fb-c3f4b2e8c96b",
   "metadata": {},
   "source": [
    "**domain similarity tests**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4db2757-7de2-4821-9d7d-b71594845e7d",
   "metadata": {},
   "source": [
    "names of the 7 domains of the candidate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af815d0-3055-4f5c-8c73-760a98998002",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['amz', 'wiki', 'news', 'pubmed', 'arxiv', 'book1', 'owtc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4695b7b0-955f-441d-9df9-44ad338723f7",
   "metadata": {},
   "source": [
    "Optimal Transport is computed on GPU with 'jax' using package 'ott-jax'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01145132-6e6b-4a40-98b4-912497f259e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import os\n",
    "\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = \"false\"\n",
    "\n",
    "print(jax.numpy.ones(3).device()) # TFRT_CPU_0\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import ott\n",
    "from ott.geometry import pointcloud\n",
    "from ott.problems.linear import linear_problem\n",
    "from ott.solvers.linear import sinkhorn, sinkhorn_lr\n",
    "from ott.tools import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39fde2d-5df9-4518-848f-d8646362e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ott.solvers.linear import sinkhorn_lr\n",
    "import tqdm\n",
    "from ott import utils\n",
    "from ott.solvers.linear import acceleration\n",
    "\n",
    "dists = []\n",
    "\n",
    "for i in range(7):\n",
    "    geom = pointcloud.PointCloud(np.array(embeds_task_1k), np.array(embeds_rand7_20k[i*20000:20000+i*20000]), epsilon=5e-1)\n",
    "    ot_prob = linear_problem.LinearProblem(geom)\n",
    "\n",
    "    with tqdm.tqdm() as pbar:\n",
    "        progress_fn = utils.tqdm_progress_fn(pbar)\n",
    "        solver = sinkhorn.Sinkhorn(progress_fn=progress_fn, momentum=acceleration.Momentum(value=1.2), threshold = 5e-2, inner_iterations=1, max_iterations = 2000)\n",
    "        # solver = sinkhorn.Sinkhorn(progress_fn=progress_fn, inner_iterations=1, max_iterations = 200000)\n",
    "        # momentum=None\n",
    "        ot_u = jax.jit(solver)(ot_prob)\n",
    "    \n",
    "    print(i, names[i], f\"Converged: {ot_u.converged}, cost: {ot_u.reg_ot_cost}\")\n",
    "    \n",
    "        # transp_cost = ot_lr.compute_reg_ot_cost(ot_prob, use_danskin=True)\n",
    "    transp_cost = ot_u.reg_ot_cost\n",
    "    dists.append(transp_cost)\n",
    "    # dis_cola[i] = transp_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8562de-dbba-4264-9034-c9d371bf5189",
   "metadata": {},
   "source": [
    "Based on the OT distances, select domains with the smallest OT distances to the target task data and use samples from these domains as the candidate data for further selection. Domains with large OT distances will be discarded for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab255aa2-d9a4-48e6-8a5f-9146e0571d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p39h",
   "language": "python",
   "name": "p39h"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

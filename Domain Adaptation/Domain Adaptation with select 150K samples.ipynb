{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download your corpus for stage 1 (pre-training on selected data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c04d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download target datasets (stage 2) from https://github.com/allenai/dont-stop-pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28e3184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#another command to download said target datasets\n",
    "# !curl -Lo reviews_amazon_test.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/amazon/test.jsonl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7ae35a",
   "metadata": {},
   "source": [
    "# MLM training - stage 1: Pre-training on selected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5e8ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adaa4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352a67bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import pickle\n",
    "import lzma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486fc5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters for MLM fine-tuning\n",
    "model_name = \"bert-base-uncased\"\n",
    "batch_size = 64\n",
    "epochs_mlm = 1\n",
    "learning_rate_mlm = 1e-4\n",
    "data_path_mlm = 'imdb_150k.pkl'  # Update with your selected data's path for pre-training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3c102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load selected pre-training data \n",
    "\n",
    "#an example of loading input data for pickled datasets\n",
    "\n",
    "import gzip\n",
    "# Load and preprocess your data for MLM fine-tuning\n",
    "with open(data_path_mlm, 'rb') as file:\n",
    "    data_mlm = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97867da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the selected pre-training dataset. We choose a max length of 256\n",
    "\n",
    "tokenizer_mlm = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "inputs_mlm = tokenizer_mlm(data_mlm, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "data_collator_mlm = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer_mlm,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15,  # Mask 15% of tokens,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Create a training configuration for MLM fine-tuning\n",
    "training_args_mlm = TrainingArguments(\n",
    "    output_dir=\"./bert_mlm_finetuned_50k_imdb\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=epochs_mlm,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    save_steps=5000,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=learning_rate_mlm\n",
    ")\n",
    "\n",
    "# Load the pre-trained BERT model for MLM fine-tuning\n",
    "model_mlm = AutoModelForMaskedLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b20bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert tokenized data to Pyarrow style Dataset ( HuggingFace Datasets)\n",
    "from datasets import Dataset, DatasetDict\n",
    "dataset = Dataset.from_dict(inputs_mlm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c8acf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a Trainer instance for MLM fine-tuning\n",
    "#This trainer uses Parallel GPU processing\n",
    "trainer_mlm = Trainer(\n",
    "    model=model_mlm,\n",
    "    args=training_args_mlm,\n",
    "    data_collator=data_collator_mlm,\n",
    "    train_dataset=dataset\n",
    "    \n",
    ")\n",
    "\n",
    "# Fine-tune the model for MLM \n",
    "trainer_mlm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3edb9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_mlm.save_model(\"./bert_mlm_finetuned_150K_imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3287354f",
   "metadata": {},
   "source": [
    "# Fine-tuning on downstream classification tasks - Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037b0282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding, Trainer\n",
    "\n",
    "# Define hyperparameters for classification fine-tuning\n",
    "epochs_classification = 10\n",
    "learning_rate_classification = 1e-4\n",
    "batch_size_classification = 64\n",
    "model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af3403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "data_path_classification = 'reviews_imdb_train.jsonl'  # Update with your classification data path to the downloaded target datasets\n",
    "test_data_path = 'reviews_imdb_test.jsonl' \n",
    "\n",
    "# Load and preprocess classification data\n",
    "classification_data = load_dataset('json', data_files=data_path_classification)\n",
    "classification_test_data = load_dataset('json', data_files=test_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed2ec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the train and test sets\n",
    "tokenizer_classification = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "tokenized_data_classification = tokenizer_classification(\n",
    "    list(classification_data['train']['text']),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=256,  # Adjust max_length as needed\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "\n",
    "tokenized_test_data_classification = tokenizer_classification(\n",
    "    list(classification_test_data['train']['text']),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=256,  # Adjust max_length as needed\n",
    "    return_tensors=\"pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f14571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id2label=dict([(i, x) for i, x in enumerate(list(np.unique(classification_data['train']['label'])))])\n",
    "label2id= dict([(x,i) for i, x in enumerate(list(np.unique(classification_data['train']['label'])))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ee0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store label information in a separate variable\n",
    "train_label=classification_data['train']['label']\n",
    "for i in range(len(train_label)):\n",
    "    train_label[i]=label2id[train_label[i]]\n",
    "dev_label=classification_dev_data['train']['label']\n",
    "for i in range(len(dev_label)):\n",
    "    dev_label[i]=label2id[dev_label[i]]    \n",
    "test_label=classification_test_data['train']['label']\n",
    "for i in range(len(test_label)):\n",
    "    test_label[i]=label2id[test_label[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113668e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification model from weights obtained after MLM pre-training our model\n",
    "model_classification = AutoModelForSequenceClassification.from_pretrained(\"./bert_mlm_finetuned_150K_imdb\",\n",
    "                                                                          num_labels=2, \n",
    "#                                                                           id2label=id2label, \n",
    "#                                                                           label2id=label2id\n",
    "                                                                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d1d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the tokenized target datasets to Huggingface Datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset_classification = Dataset.from_dict(\n",
    "    {\n",
    "        'input_ids': tokenized_data_classification['input_ids'],\n",
    "        'attention_mask': tokenized_data_classification['attention_mask'],\n",
    "        'labels':train_label\n",
    "    }\n",
    ")\n",
    "\n",
    "dev_dataset_classification = Dataset.from_dict(\n",
    "    {\n",
    "        'input_ids': tokenized_dev_data_classification['input_ids'],\n",
    "        'attention_mask': tokenized_dev_data_classification['attention_mask'],\n",
    "        'labels':dev_label\n",
    "    }\n",
    ")\n",
    "\n",
    "test_dataset_classification = Dataset.from_dict(\n",
    "    {\n",
    "        'input_ids': tokenized_test_data_classification['input_ids'],\n",
    "        'attention_mask': tokenized_test_data_classification['attention_mask'],\n",
    "        'labels': test_label\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa225bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set compute Metrics to F1-score. Note that Biomed Domain uses 'micro' F1 score, the rest use 'macro'\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"f1\")\n",
    "#f1 score\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels,average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc983715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data collator for classification\n",
    "data_collator_classification = DataCollatorWithPadding(tokenizer_classification, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f2470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TrainingArguments object for classification fine-tuning\n",
    "training_args_classification = TrainingArguments(\n",
    "    output_dir=\"./bert_classification\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=epochs_classification,\n",
    "    per_device_train_batch_size=batch_size_classification,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=learning_rate_classification,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,  \n",
    "    logging_steps=100,  \n",
    "    seed=43,\n",
    "    load_best_model_at_end=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01631a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Create a Trainer instance for classification fine-tuning\n",
    "trainer_classification = Trainer(\n",
    "    model=model_classification,\n",
    "    args=training_args_classification,\n",
    "    data_collator=data_collator_classification,\n",
    "    train_dataset=train_dataset_classification,\n",
    "    eval_dataset=test_dataset_classification,  # Use the test dataset for evaluation\n",
    "    compute_metrics =compute_metrics\n",
    ")\n",
    "\n",
    "# Fine-tune the model for classification\n",
    "trainer_classification.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a405f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Evaluate the best final model over the test set for performance\n",
    "trainer_classification.evaluate(test_dataset_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149be41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the best model\n",
    "trainer_classification.save_model(\"./bert_classification/reviews_imdb_150K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edf9c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
